from typing import Mapping, List

import pandas as pd
from tqdm import tqdm
from main_classificator import ct
from random import shuffle
import numpy as np

from main_classificator.preprocessing.files_manipulation import get_all_file_names, read_data_from_files, \
    get_num_of_files
from main_classificator.utils import Extention, get_file_name_by_extention


def read_data_in_dict(path_to_folder: str,
                      num_of_files: int = None,
                      from_num: int = None,
                      ext: Extention = None,
                      shuffle_list: bool = False) -> Mapping[str, str]:
    file_names = get_all_file_names(path_to_folder)

    if ext:
        file_names = get_file_name_by_extention(file_names, ext)

    if shuffle_list:
        shuffle(file_names)

    if from_num:
        assert from_num >= 0, f"wrong num of from_num parameter = {from_num}"
    else:
        from_num = 0

    if num_of_files:
        assert num_of_files > 0, f"wrong num of files = {num_of_files}"
        file_names = file_names[from_num:from_num + num_of_files]

    all_data = read_data_from_files(file_names)
    return all_data


def transform_bytes_into_np_array(all_data: Mapping[str, str]) -> Mapping[str, np.array]:
    def get_bytes(byte_file: str) -> np.array:
        """

        :param byte_file: str with bytes from file
            template for a row - num_of_a_rowI byte1 byte2 ... byteN\n

            delimeters:
            \n - for rows
            ' ' - for bytes and num of row
        :return:
            for each file_name will be np.array with bytes. -1 is representing QM '?', next num - num of ? goes next
        """

        rows = [cutted_row.split(' ')[1:] for cutted_row in byte_file.split('\n')]
        just_bytes = []
        [just_bytes.extend(r) for r in rows]
        int_bytes = []
        num_of_QM = 0

        for byte in just_bytes:
            if '?' not in byte:
                if num_of_QM != 0:
                    int_bytes.append(-1)
                    int_bytes.append(num_of_QM)
                    num_of_QM = 0

                int_byte = int(byte, 16)
                int_bytes.append(int_byte)
            else:
                num_of_QM += 1

        return np.array(int_bytes)

    transformed_data = {}
    for k, v in all_data.items():
        transformed_data[k] = get_bytes(v)
    return transformed_data


def get_dataset_from_files(path_to_input_folder: str,
                           num_of_files: int = None,
                           from_num: int = None,
                           ext: Extention = None,
                           shuffle_list: bool = False):
    all_data = read_data_in_dict(path_to_folder=path_to_input_folder,
                                 num_of_files=num_of_files,
                                 from_num=from_num,
                                 ext=ext,
                                 shuffle_list=shuffle_list)

    np_data = transform_bytes_into_np_array(all_data)
    return np_data


def get_min_file_len(path_to_folder: str) -> int:
    num_of_files = get_num_of_files(path_to_folder)
    step = 1
    min_len = None
    for i in range(0, num_of_files, step):
        np_dataset = get_dataset_from_files(path_to_input_folder=path_to_folder,
                                            num_of_files=step,
                                            from_num=i,
                                            ext=Extention.BYTES,
                                            shuffle_list=False)
        if not list(np_dataset.values()):
            break
        assert len(list(np_dataset.values())) == 1
        cur_len = len(list(np_dataset.values())[0])
        if min_len is None:
            min_len = cur_len
        elif cur_len < min_len:
            min_len = cur_len
    return min_len


def fit_file_by_size(np_data: Mapping[str, np.array], min_len: int, overlap: int) -> pd.DataFrame:
    # TODO: check this, if it gives bad results, then just use reshape
    res_df = pd.DataFrame()
    for file_name, cur_bytes in np_data.items():
        cur_bytes = cur_bytes.tolist()
        cur_res = []
        len_of_bytes = len(cur_bytes)
        for i in range(0, len_of_bytes, min_len-overlap):
            cur_dataset = cur_bytes[i:i+min_len]
            if len(cur_dataset) == min_len:
                cur_res.append(cur_dataset)
            else:
                dataset_to_check = cur_bytes[len_of_bytes - min_len:len_of_bytes]
                if not np.array_equal(dataset_to_check, cur_res[-1]):
                    cur_res.append(dataset_to_check)
        res_df = pd.concat([res_df, pd.DataFrame({ct.ID_COLUMN: [file_name]*len(cur_res),
                                                  ct.BYTES_COLUMN: cur_res})])
    return res_df


def write_preprocessed_data(path_to_input_folder: str,
                            path_to_output_file: str,
                            min_len: int,
                            overlap: int,
                            num_of_files: int = None,
                            from_num: int = None,
                            ext: Extention = None,
                            shuffle_list: bool = False):

    np_data = get_dataset_from_files(path_to_input_folder, num_of_files, from_num, ext, shuffle_list)
    res_df = fit_file_by_size(np_data, min_len, overlap)
    res_df.to_csv(path_to_output_file)


def read_batches_from_train_preprocessed(nums_of_batches_to_read: List[int]) -> pd.DataFrame:
    assert nums_of_batches_to_read
    for batch_num in nums_of_batches_to_read:
        yield pd.read_csv(ct.get_path_to_train_preprocessed_file(batch_num))


def preproc_dataset_in_batches():
    # m = get_min_file_len(ct.get_path_to_train_cut_folder())
    # print(m)
    pre_calculated_min_len = 6492
    overlap = 1000
    num_of_files = get_num_of_files(ct.get_path_to_train_cut_folder(), ext=Extention.BYTES)
    step = 3
    num_of_batch = 0
    for i in tqdm(range(0, num_of_files, step)):
        write_preprocessed_data(path_to_input_folder=ct.get_path_to_train_cut_folder(),
                                path_to_output_file=ct.get_path_to_train_preprocessed_file(num_of_batch),
                                min_len=pre_calculated_min_len,
                                overlap=overlap,
                                num_of_files=step,
                                from_num=i,
                                ext=Extention.BYTES,
                                shuffle_list=False)
        num_of_batch += 1


if __name__ == '__main__':
    res_df = pd.DataFrame()
    for df in read_batches_from_train_preprocessed(list(range(65))):
        res_df = pd.concat([res_df, df])
    print(len(res_df))
