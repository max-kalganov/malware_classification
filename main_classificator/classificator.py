from typing import List, Tuple

import pandas as pd
from sklearn.neural_network import MLPClassifier
import numpy as np
import torch.nn as nn
import torch
from torch.autograd import Variable

from tqdm import tqdm
from main_classificator import ct
from main_classificator.preprocessing.preproc_data import prepare_train_dataset_pipeline, \
    read_batches_from_test_preprocessed, get_x


def small_batch_classificator():
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes*0.5),
                          int(len_of_bytes*0.2),
                          9)
    clf = simple_model(X, Y, hidden_layers_size=hidden_layers_size)
    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def large_batch_classification():
    # doesn't work, need to reprepare train data, so in each batch will be all classes
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes * 0.5),
                          int(len_of_bytes * 0.2),
                          9,)

    clf = MLPClassifier(activation='tanh', solver='sgd', learning_rate='constant',
                        alpha=1e-4, hidden_layer_sizes=hidden_layers_size, random_state=1,
                        batch_size=1, verbose=False,
                        max_iter=1, warm_start=True)

    clf = clf.partial_fit(X, Y, classes=[1, 2, 3, 4, 5, 6, 7, 8, 9])

    print("start fitting main train dataset")

    for num in tqdm(range(0, 3256)):
        X, Y = prepare_train_dataset_pipeline([num],
                                              path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        clf = clf.partial_fit(X, Y)

    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def aggregate_results(df: pd.DataFrame) -> pd.DataFrame:
    assert ct.PREDICTION_COLUMN in df.columns and ct.ID_COLUMN in df.columns

    # TODO: use a classificator here to get the result classification

    def proc_groups(name, group):
        result_prediction = sum(np.array(list(group[ct.PREDICTION_COLUMN].values))) / len(group)
        return pd.DataFrame({ct.ID_COLUMN: [name],
                             ct.PREDICTION_COLUMN: [result_prediction.tolist()]})

    df = pd.concat([proc_groups(name, group) for name, group in df.groupby([ct.ID_COLUMN])])
    return df


def predict_large(clf) -> pd.DataFrame:
    result_df = pd.DataFrame()
    for df in tqdm(read_batches_from_test_preprocessed(list(range(ct.NUM_OF_TEST_BATCHES)))):
        X_test = get_x(df)
        prediction = clf.predict_proba(X_test)
        df[ct.PREDICTION_COLUMN] = prediction.tolist()
        result_df = pd.concat([result_df, aggregate_results(df)])
    return result_df


def simple_model(X, Y, hidden_layers_size = (5,2)):
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=hidden_layers_size, random_state=1)

    clf.fit(X, Y)
    return clf


def simple_example():
    from random import randint
    # X_train = [[-1., -2.]]
    # y = [0]
    X_train = []
    y = []

    for i in range(100):
        x_ = randint(-10, 10)
        y_ = randint(-10, 10)
        X_train.append([x_, y_])
        if y_ == x_:
            y.append(2)
        else:
            y.append(int(y_ - x_ > 0))

    clf = simple_model(X_train, Y=y, hidden_layers_size=(5, 5, 5))

    X_test = [[0., 0.], [-1., -2.], [-2., -1.], [-1., -1.], [2., 1.]]
    prediction = clf.predict(X_test)
    y_test = [2, 0, 1, 2, 0]
    print(f"prediction result = {prediction}: result is correct - {np.array_equal(prediction, np.array(y_test))}")




# ///////////////////////

class BatchGenerator:

    def __init__(self, input_len=10, batch_size=1, num_of_batches=1):
        self.batch_size = batch_size
        self.num_of_batches = num_of_batches
        self.counter = 0

    def __next__(self):
        if self.counter < self.num_of_batches:
            X, Y = prepare_train_dataset_pipeline([self.counter],
                                                  path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)

            self.counter += 1
            # X = (np.random.randn(self.batch_size, self.input_len) > 0.5).astype(int)
            # Y = np.concatenate((X[:, 1:], X[:, 0:1]), axis=1)
            return X, Y
        else:
            raise StopIteration

    def __iter__(self):
        return self


class SimpleMultiClassifier(nn.Module):
    def __init__(self, hidden_layers_size: List[Tuple[int, int]]):
        super().__init__()
        self.hidden_layers_size = hidden_layers_size
        self.hidden_layer_num: int = 0
        self.model = nn.Sequential(
            nn.Linear(*self.__get_next_hidden_layer_size()),
            nn.Sigmoid(),
            nn.Linear(*self.__get_next_hidden_layer_size()),
            nn.Sigmoid()
        )
        #     .apply(self.init_weights)

    def __get_next_hidden_layer_size(self) -> Tuple[int, int]:
        next_size = self.hidden_layers_size[self.hidden_layer_num]
        self.hidden_layer_num += 1
        print(f" hidden layers size = {next_size}")
        return next_size

    def forward(self, x):
        return self.model(x)

    # @staticmethod
    # def init_weights(m):
    #     if type(m) == nn.Linear:
    #         torch.nn.init.kaiming_normal_(m.weight)


def PyorchClassification():

    hidden_layers = [(ct.MIN_WINDOW_SIZE, 100), (100, ct.NUM_OF_CLASSES)]

    net = SimpleMultiClassifier(hidden_layers_size=hidden_layers)

    criterion = nn.MSELoss()
    l_rate = 0.5
    # Stochastic Gradient Descent
    optimizer = torch.optim.SGD(net.parameters(), lr=l_rate)

    # epochs = 2000

    # print("X =  \n", X)

    # BG = BatchGenerator(input_size, 1000, 10)
    # print("Y = \n", Y)
    loss = None

    for num in tqdm(range(65)):
        X, Y = prepare_train_dataset_pipeline([num])

        # X, Y = prepare_train_dataset_pipeline([num],
        #                                       path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        inputs = Variable(torch.from_numpy(X)).float()
        labels = Variable(torch.from_numpy(Y)).float()
        optimizer.zero_grad()

        outputs = net.forward(inputs)

        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # print(f"loss {loss} ")

    print(f"loss {loss} ")

    # for epoch in tqdm(range(epochs)):
    #     for batch in BG:
    #         X = (np.random.randn(100, input_size) > 0.5)
    #         Y = np.concatenate((X[:, 1:], X[:, 0:1]), axis=1)
    #         inputs = Variable(torch.from_numpy(X.astype(int))).float()
    #         labels = Variable(torch.from_numpy(Y.astype(int))).float()
    #         optimizer.zero_grad()
    #
    #         outputs = net.forward(inputs)
    #         loss = criterion(outputs, labels)
    #         loss.backward()
    #         optimizer.step()
    #         print(f"epoch {epoch}, loss {loss} ")

    X_test, Y_test = prepare_train_dataset_pipeline([11, 12],
                                          path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)

    inputs = Variable(torch.from_numpy(X_test.astype(int))).float()
    Y = (net.forward(inputs)).float()


    #Z = (Y_test == Y)
    print(Y)
    print(Y_test[Y_test == Y.detach().numpy()])
    # print(Z[Z != 1])


if __name__ == '__main__':
    PyorchClassification()
