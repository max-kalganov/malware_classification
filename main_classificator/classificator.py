from dataclasses import dataclass
from enum import Enum, auto
from typing import List, Tuple
from random import randint

import pandas as pd
from sklearn.neural_network import MLPClassifier
import numpy as np
import torch.nn as nn
import torch
from torch.autograd import Variable

from tqdm import tqdm
from main_classificator import ct
from main_classificator.preprocessing.preproc_data import prepare_train_dataset_pipeline, \
    read_batches_from_test_preprocessed, get_x
from main_classificator.utils import format_batches
import torch.nn.functional as F


def small_batch_classificator():
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes*0.5),
                          int(len_of_bytes*0.2),
                          9)
    clf = simple_model(X, Y, hidden_layers_size=hidden_layers_size)
    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def large_batch_classification():
    # doesn't work, need to reprepare train data, so in each batch will be all classes
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes * 0.5),
                          int(len_of_bytes * 0.2),
                          9,)

    clf = MLPClassifier(activation='tanh', solver='sgd', learning_rate='constant',
                        alpha=1e-4, hidden_layer_sizes=hidden_layers_size, random_state=1,
                        batch_size=1, verbose=False,
                        max_iter=1, warm_start=True)

    clf = clf.partial_fit(X, Y, classes=[1, 2, 3, 4, 5, 6, 7, 8, 9])

    print("start fitting main train dataset")

    for num in tqdm(range(0, 3256)):
        X, Y = prepare_train_dataset_pipeline([num],
                                              path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        clf = clf.partial_fit(X, Y)

    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def aggregate_results(df: pd.DataFrame) -> pd.DataFrame:
    assert ct.PREDICTION_COLUMN in df.columns and ct.ID_COLUMN in df.columns

    # TODO: use a classificator here to get the result classification

    def proc_groups(name, group):
        result_prediction = sum(np.array(list(group[ct.PREDICTION_COLUMN].values))) / len(group)
        return pd.DataFrame({ct.ID_COLUMN: [name],
                             ct.PREDICTION_COLUMN: [result_prediction.tolist()]})

    df = pd.concat([proc_groups(name, group) for name, group in df.groupby([ct.ID_COLUMN])])
    return df


def predict_large(clf) -> pd.DataFrame:
    result_df = pd.DataFrame()
    for df in tqdm(read_batches_from_test_preprocessed(list(range(ct.NUM_OF_TEST_BATCHES)))):
        X_test = get_x(df)
        prediction = clf.predict_proba(X_test)
        df[ct.PREDICTION_COLUMN] = prediction.tolist()
        result_df = pd.concat([result_df, aggregate_results(df)])
    return result_df


def simple_model(X, Y, hidden_layers_size = (5,2)):
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=hidden_layers_size, random_state=1)

    clf.fit(X, Y)
    return clf


def simple_example():
    from random import randint
    # X_train = [[-1., -2.]]
    # y = [0]
    X_train = []
    y = []

    for i in range(100):
        x_ = randint(-10, 10)
        y_ = randint(-10, 10)
        X_train.append([x_, y_])
        if y_ == x_:
            y.append(2)
        else:
            y.append(int(y_ - x_ > 0))

    clf = simple_model(X_train, Y=y, hidden_layers_size=(5, 5, 5))

    X_test = [[0., 0.], [-1., -2.], [-2., -1.], [-1., -1.], [2., 1.]]
    prediction = clf.predict(X_test)
    y_test = [2, 0, 1, 2, 0]
    print(f"prediction result = {prediction}: result is correct - {np.array_equal(prediction, np.array(y_test))}")


####################################
# Writing PyTorch Classificator
####################################
class CommonClassifierTemplate(nn.Module):
    def __init__(self, hidden_layers_size: List[Tuple[int, int]]):
        super().__init__()
        self.hidden_layers_size = hidden_layers_size
        self.hidden_layer_num: int = 0

    def get_next_hidden_layer_size(self) -> Tuple[int, int]:
        next_size = self.hidden_layers_size[self.hidden_layer_num]
        self.hidden_layer_num += 1
        print(f" hidden layers size = {next_size}")
        return next_size


class MainCNN(CommonClassifierTemplate):
    def __init__(self, hidden_layers_size: List[Tuple[int, int]], first_linear_input: int):
        super().__init__(hidden_layers_size)
        self.first_linear_input = first_linear_input

        self.conv1 = nn.Conv1d(*self.get_next_hidden_layer_size())
        self.pool1 = nn.MaxPool1d(*self.get_next_hidden_layer_size())
        self.conv2 = nn.Conv1d(*self.get_next_hidden_layer_size())
        self.pool2 = nn.MaxPool1d(*self.get_next_hidden_layer_size())
        self.lin1 = nn.Linear(*self.get_next_hidden_layer_size())
        self.drop = nn.Dropout()
        self.lin2 = nn.Linear(*self.get_next_hidden_layer_size())
        self.lin3 = nn.Linear(*self.get_next_hidden_layer_size())

    def forward(self, x):
        x = x.reshape(x.shape[0], 1, ct.MIN_WINDOW_SIZE)
        x = F.relu(self.conv1(x))
        x = self.pool1(x)

        #x = F.softmax(x)
        x = F.relu(self.conv2(x))
        x = self.pool2(x)

        x = x.view(-1, self.first_linear_input)
        x = torch.sigmoid(self.lin1(x))
        x = self.drop(x)
        x = torch.sigmoid(self.lin2(x))
        x = torch.sigmoid(self.lin3(x))
        return x


class SimpleCNN(torch.nn.Module):
    def __init__(self):
        super().__init__()
        # Input channels = 1, output channels = 10
        self.conv1 = torch.nn.Conv1d(1, 10, kernel_size=3, stride=1, padding=1)
        self.pool = torch.nn.MaxPool1d(kernel_size=3, stride=0, padding=0)

        # 21670 input features, 3200 output features (see sizing flow below)
        self.fc1 = torch.nn.Linear(int(10 * ct.MIN_WINDOW_SIZE / 3), ct.BEST_MID_LAYER)
        # 3200 input features, 9 output features for our 9 defined classes
        self.fc2 = torch.nn.Linear(ct.BEST_MID_LAYER, 9)

    def forward(self, x):
        # Computes the activation of the first convolution
        x = x.reshape(x.shape[0], 1, 6501)
        # Size changes from (1, 6501) to (10, 6501)
        x = F.relu(self.conv1(x))
        # Size changes from (10, 6501) to (10, 2167)
        x = self.pool(x)

        # Reshape data to input to the input layer of the neural net
        # Size changes from (10, 2167) to (1, 21670)
        # Recall that the -1 infers this dimension from the other given dimension
        x = x.view(-1, 10 * ct.MIN_WINDOW_SIZE // 3)

        # Computes the activation of the first fully connected layer
        # Size changes from (1, 21670) to (1, 3200)
        x = F.relu(self.fc1(x))

        # Computes t    he second fully connected layer (activation applied later)
        # Size changes from (1, 3200) to (1, 9)
        x = self.fc2(x)
        x = torch.sigmoid(x)
        return x


class SimpleMultiClassifier(CommonClassifierTemplate):
    def __init__(self, hidden_layers_size: List[Tuple[int, int]]):
        super().__init__(hidden_layers_size)
        self.model = nn.Sequential(
            nn.Linear(*self.get_next_hidden_layer_size()),
            nn.Sigmoid(),
            nn.Linear(*self.get_next_hidden_layer_size()),
            nn.Sigmoid(),
        )

    def forward(self, x):
        return self.model(x)

    @staticmethod
    def init_weights(m):
        if type(m) == nn.Linear:
            torch.nn.init.kaiming_normal_(m.weight)


@dataclass
class ClassifArgs:
    def __init__(self,
                 num_of_batches: int,
                 hidden_layers: List[Tuple[int, int]],
                 criterion,
                 first_linear_input: int,
                 l_rate: float,
                 percent_of_tasks_to_train: float = 0.75):

        assert num_of_batches > 0
        assert 0 < percent_of_tasks_to_train <= 1, f"percent not in range (0, 1]"
        self.num_of_batches = num_of_batches
        self.hidden_layers = hidden_layers
        self.criterion = criterion
        self.first_linear_input = first_linear_input
        self.l_rate = l_rate
        self.num_to_train = int(self.num_of_batches * percent_of_tasks_to_train)
        self.num_to_test = self.num_of_batches - self.num_to_train


def predict_large_pytorch(pytorch_NN) -> pd.DataFrame:
    result_df = pd.DataFrame()
    for df in tqdm(read_batches_from_test_preprocessed(list(range(ct.NUM_OF_TEST_BATCHES)))):
        X_test = get_x(df)

        inputs = Variable(torch.from_numpy(X_test.astype(int))).float()
        prediction = (pytorch_NN.forward(inputs)).float()

        prediction = prediction.cpu().detach().numpy()
        df[ct.PREDICTION_COLUMN] = prediction.tolist()
        result_df = pd.concat([result_df, aggregate_results(df)])
    return result_df


def pytorch_classification(classif_args: ClassifArgs,
                           full_train: bool = False,
                           predict: bool = False,
                           get_in_train_for_stats: List[int] = None,
                           no_printing: bool = False,
                           step_for_training: int = 1):
    def get_path_generator():
        if full_train:
            path_generator = ct.get_path_to_train_full_preprocessed_file
        else:
            path_generator = ct.get_path_to_train_preprocessed_file
        return path_generator

    def train():
        def get_range(cur_num: int, max_num: int, cur_step: int):
            if cur_num <= max_num - cur_step:
                result = list(range(cur_num, cur_num + cur_step))
            else:
                result = list(range(cur_num, max_num))
            return result

        loss = None
        step = step_for_training
        my_range = range(0, classif_args.num_to_train, step)
        if not no_printing:
            my_range = tqdm(my_range)

        for num in my_range:
            X, Y = prepare_train_dataset_pipeline(get_range(num, classif_args.num_to_train, step),
                                                  path_to_train_file_generator=get_path_generator())

            inputs = Variable(torch.from_numpy(X)).float()
            labels = Variable(torch.from_numpy(Y)).float()
            optimizer.zero_grad()

            outputs = pytorch_NN.forward(inputs)

            loss = classif_args.criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        my_print(f"loss {loss} ")

    def test():
        if not get_in_train_for_stats:
            get_for_test_batches = list(range(classif_args.num_to_train, classif_args.num_of_batches))
            path_generator = get_path_generator()
        else:
            get_for_test_batches = get_in_train_for_stats
            path_generator = ct.get_path_to_train_full_preprocessed_file

        X_test, Y_test = prepare_train_dataset_pipeline(
            get_for_test_batches,
            path_to_train_file_generator=path_generator)

        inputs = Variable(torch.from_numpy(X_test.astype(int))).float()
        Y = (pytorch_NN.forward(inputs)).float()

        my_print(Y)
        np_y = Y.cpu().detach().numpy()
        predictions = np.argmax(np_y, axis=1)
        Y_test = np.argmax(Y_test, axis=1)
        my_print(f"num of correct answers = {Y_test[Y_test == predictions].shape} from {Y_test.shape}")
        accuracy = Y_test[Y_test == predictions].shape[0] / Y_test.shape[0]
        return accuracy

    # Review:
    # pytorch_NN = SimpleMultiClassifier(hidden_layers_size=classif_args.hidden_layers)
    pytorch_NN = MainCNN(hidden_layers_size=classif_args.hidden_layers,
                         first_linear_input=classif_args.first_linear_input)
    if no_printing:
        my_print = lambda x: None
    else:
        my_print = print

    # Stochastic Gradient Descent
    optimizer = torch.optim.SGD(pytorch_NN.parameters(), lr=classif_args.l_rate)

    train()

    torch.save(pytorch_NN.state_dict(), ct.get_model_new_file())

    accuracy = None
    if classif_args.num_to_test > 0 or get_in_train_for_stats:
        accuracy = test()

    if predict:
        predictions = predict_large_pytorch(pytorch_NN)
        predictions = format_batches(predictions)
        predictions.to_csv(ct.get_predictions_new_file())

    return accuracy


def predict_with_load(path_to_model: str, classif_args: ClassifArgs):
    model = MainCNN(hidden_layers_size=classif_args.hidden_layers,
                    first_linear_input=classif_args.first_linear_input)
    model.load_state_dict(torch.load(path_to_model))
    model.eval()
    predictions = predict_large_pytorch(model)
    predictions = format_batches(predictions)
    predictions.to_csv(ct.get_predictions_new_file())


if __name__ == '__main__':
    # for df in tqdm(read_batches_from_test_preprocessed(list(range(3300, 3500)))):
    #     X_test = get_x(df)

    classif_args = ClassifArgs(
                num_of_batches=ct.TRAIN_FULL_PREPROCESSED_SIZE,
                hidden_layers=[(1, 5, 3, 1, 1),  # in_shape = (batch_size, 1, 6501), out_shape = (batch_size, 10, 6501)
                               (3, 0, 0),  # in_shape = (batch_size, 10, 6501), out_shape = (batch_size, 10, 2167)
                               (5, 10, 3, 1, 1), # in_shape = (batch_size, 10, 2167), out_shape = (batch_size, 20, 2167)
                               (3, 0, 0),  # in_shape = (batch_size, 20, 2167), out_shape = (batch_size, 20, 722)
                               (10 * 722, 5000),
                               (5000, ct.BEST_MID_LAYER),
                               (ct.BEST_MID_LAYER, ct.NUM_OF_CLASSES)],
                criterion=nn.MSELoss(),
                l_rate=ct.BEST_ALPHA,
                percent_of_tasks_to_train=1,
                first_linear_input=10*722) # 1)
    # pytorch_classification(classif_args, full_train=True, predict=True)
    predict_with_load(path_to_model=ct.get_model_folder_path() + 'model_1558196216.pt',
                      classif_args=classif_args)
