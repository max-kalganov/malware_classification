from collections import defaultdict
import operator
import gc
from main_classificator.ct import PATH_TO_MALWARE_DATASET_FOLDER
"""

Method to detect different bytes values and update the given dictionary

The aim is to create a corpus with all unique words. 

"""


def convertfiletotokenarray(id_to_open, worddict):
    words_file = defaultdict(lambda: 0)
    fop = open(id_to_open.replace('"', '') + '.bytes', 'r')  # open the bytes file
    for mline in fop:
        # use whole bytes as grams
        str_to_pass = mline.replace("\n", "")  # break by space
        if str_to_pass not in words_file:  # if word not present in current dictionary...add the term of the word in  worddict
            words_file[str_to_pass] += 1
            worddict[str_to_pass] += 1


"""
based on a dictionary of words and given ngrams, prints teh counts of bytes in the given file .
"""


def convertfiletotokenarraybytetokens(id_to_open, indexesdict, files):
    words_file = defaultdict(lambda: 0)
    fop = open(id_to_open.replace('"', '') + '.bytes', 'r')  # open the bytes file
    row_length = 0
    total_elements = 0
    for mline in fop:
        row_length += 1
        str_to_pass = mline.replace("\n", "")
        if str_to_pass in indexesdict:  # if word not present in current dictionary...add the term of the word in  worddict
            words_file[str_to_pass] += 1
            total_elements += 1

    files.write(str(row_length) + "," + str(total_elements))
    for word, ind in indexesdict.items():
        if word in words_file:
            bvalue = float(words_file[word])
            files.write("," + str(bvalue))
        else:
            files.write(",0")
    files.write("\n")


"""
method that creates ngrams from bytes and includes 2 steps:

1) scan teh files and the fine the bytes ("words")
2) re-scan files and print new train and test sets with counts' distribution of the bytes' founds via teh scanning process
parameters:

trainfile= the labels' file (becasuse we can get the ids)
testfile= the sample submission file (becasuse we can get the ids for the tets cases)
ngrams= number of ngrams. This considers the number of grands strictly. e.g ngrams=2 considers only ngrams 2 (not 1 grams too)
output_name= prefix for the train and test files

"""


def makesets(trainfile="trainLabels.csv", testfile="sampleSubmission.csv", output_name=""):
    # wordsother=loaddicts("E:/" + "dictspersubject"  , 1000000)
    words = defaultdict(lambda: 0)
    indices = defaultdict(lambda: 0)
    words_test = defaultdict(lambda: 0)

    print("openning: " + str(trainfile))
    tr = open(trainfile, 'r')  # open training labels' file taht has the ids
    tr.readline()  # headers
    train_counter = 0
    for line in tr:  # for each line in the file
        splits = line.split(",")
        trid = splits[0]  # e.g. a key
        convertfiletotokenarray(PATH_TO_MALWARE_DATASET_FOLDER + 'train/' + trid, words)
        train_counter += 1
        if train_counter % 100 == 0:
            print("we are at train : " + str(train_counter) + " length: " + str(len(words)))
        if len(words) > 20000000.0: # 50000000.0
            print(" reached maximum number of cases: " + str(len(words)))
            break
    tr.close()
    print(" finished training tf with total distinct words: " + str(len(words)))

    print("sorting...")
    word_sorted = sorted(words.items(), key=operator.itemgetter(1), reverse=True)  # reverse sort word by count
    words = None
    gc.collect()  # call garbage collector to release some memory

    # do the same with test as this time we work with samples and want to make certain that the words exist in both
    print("openning: " + str(testfile))
    te = open(testfile, 'r')  # open training labels' file taht has the ids
    te.readline()  # headers
    test_counter = 0
    for line in te:  # for each line in the file
        splits = line.split(",")
        teid = splits[0]  # e.g. a key
        convertfiletotokenarray(PATH_TO_MALWARE_DATASET_FOLDER + 'test/' + teid, words_test)
        print(str(test_counter) + " case: " + str(splits[0]))
        test_counter += 1
        if test_counter % 100 == 0:
            print("we are at test : " + str(test_counter) + " length: " + str(len(words_test)))

        if len(words_test) > 20000000.0:
            print(" reached maximum number of cases: " + str(len(words_test)))
            break
    te.close()

    index = 2
    mini_c = 0
    thress = 40000  # number of most popular ngrams to consider
    for iu in range(0, len(word_sorted)):
        word = word_sorted[iu][0]
        if mini_c > thress:
            break
        if word in words_test:  # if word is also in the test dictionary
            indices[word] = index
            index += 1
            mini_c += 1

    # rest dictionaries
    words_test = None

    word_sorted = None
    gc.collect()  # call garbage collector to release some memory

    print(" max index is: " + str(index))

    # create train set elements

    trs = open(PATH_TO_MALWARE_DATASET_FOLDER + output_name + "train.csv", "w")
    print("openning: " + str(trainfile))
    tr = open(trainfile, 'r')  # open training labels' file taht has the ids
    tr.readline()  # headers
    train_counter = 0
    for line in tr:  # for each line in the file
        splits = line.split(",")
        trid = splits[0]  # e.g. a key
        convertfiletotokenarraybytetokens(PATH_TO_MALWARE_DATASET_FOLDER + 'train/' + trid, indices, trs)  # print line in file

        train_counter += 1
        if train_counter % 100 == 0:
            print("we are at : " + str(train_counter))
    print("create file with rows: " + str(train_counter))

    tr.close()
    trs.close()

    tes = open(PATH_TO_MALWARE_DATASET_FOLDER + output_name + "test.csv", "w")
    print("openning: " + str(testfile))
    te = open(testfile, 'r')  # open training labels' file taht has the ids
    te.readline()  # headers
    test_counter = 0
    for line in te:  # for each line in the file
        splits = line.split(",")
        teid = splits[0]  # e.g. a key
        convertfiletotokenarraybytetokens(PATH_TO_MALWARE_DATASET_FOLDER + 'test/' + teid, indices, tes)  # print line in file
        test_counter += 1
        if test_counter % 100 == 0:
            print("we are at test : " + str(test_counter))
    print("create file with rows: " + str(test_counter))

    te.close()
    tes.close()


def main():
    makesets(trainfile=PATH_TO_MALWARE_DATASET_FOLDER + "trainLabels.csv",
             testfile=PATH_TO_MALWARE_DATASET_FOLDER + "sampleSubmission.csv",
             output_name="fullinebyte")


if __name__ == "__main__":
    main()
