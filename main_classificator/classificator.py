import pandas as pd
from sklearn.neural_network import MLPClassifier
import numpy as np
from tqdm import tqdm
from main_classificator import ct
from main_classificator.preprocessing.preproc_data import prepare_train_dataset_pipeline, \
    read_batches_from_test_preprocessed, get_x


def small_batch_classificator():
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes*0.5),
                          int(len_of_bytes*0.2),
                          9)
    clf = simple_model(X, Y, hidden_layers_size=hidden_layers_size)
    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def large_batch_classification():
    # doesn't work, need to reprepare train data, so in each batch will be all classes
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes * 0.5),
                          int(len_of_bytes * 0.2),
                          9,)

    clf = MLPClassifier(activation='tanh', solver='sgd', learning_rate='constant',
                        alpha=1e-4, hidden_layer_sizes=hidden_layers_size, random_state=1,
                        batch_size=1, verbose=False,
                        max_iter=1, warm_start=True)

    clf = clf.partial_fit(X, Y, classes=[1, 2, 3, 4, 5, 6, 7, 8, 9])

    print("start fitting main train dataset")

    for num in tqdm(range(0, 3256)):
        X, Y = prepare_train_dataset_pipeline([num],
                                              path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        clf = clf.partial_fit(X, Y)

    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def aggregate_results(df: pd.DataFrame) -> pd.DataFrame:
    assert ct.PREDICTION_COLUMN in df.columns and ct.ID_COLUMN in df.columns

    # TODO: use a classificator here to get the result classification

    def proc_groups(name, group):
        result_prediction = sum(np.array(list(group[ct.PREDICTION_COLUMN].values))) / len(group)
        return pd.DataFrame({ct.ID_COLUMN: [name],
                             ct.PREDICTION_COLUMN: [result_prediction.tolist()]})

    df = pd.concat([proc_groups(name, group) for name, group in df.groupby([ct.ID_COLUMN])])
    return df


def predict_large(clf) -> pd.DataFrame:
    result_df = pd.DataFrame()
    for df in tqdm(read_batches_from_test_preprocessed(list(range(ct.NUM_OF_TEST_BATCHES)))):
        X_test = get_x(df)
        prediction = clf.predict_proba(X_test)
        df[ct.PREDICTION_COLUMN] = prediction.tolist()
        result_df = pd.concat([result_df, aggregate_results(df)])
    return result_df


def simple_model(X, Y, hidden_layers_size = (5,2)):
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=hidden_layers_size, random_state=1)

    clf.fit(X, Y)
    return clf


def simple_example():
    from random import randint
    # X_train = [[-1., -2.]]
    # y = [0]
    X_train = []
    y = []

    for i in range(100):
        x_ = randint(-10, 10)
        y_ = randint(-10, 10)
        X_train.append([x_, y_])
        if y_ == x_:
            y.append(2)
        else:
            y.append(int(y_ - x_ > 0))

    clf = simple_model(X_train, Y=y, hidden_layers_size=(5, 5, 5))

    X_test = [[0., 0.], [-1., -2.], [-2., -1.], [-1., -1.], [2., 1.]]
    prediction = clf.predict(X_test)
    y_test = [2, 0, 1, 2, 0]
    print(f"prediction result = {prediction}: result is correct - {np.array_equal(prediction, np.array(y_test))}")


if __name__ == '__main__':
    large_batch_classification()


# ///////////////////////

def PyorchClassification():
    import numpy as np
    import torch.nn as nn
    import torch
    from torch.autograd import Variable

    class BatchGenerator:

        def __init__(self, input_len=10, batch_size=1, limit=1):
            self.batch_size = batch_size
            self.limit = limit
            self.counter = 0
            self.input_len = input_len

        def __next__(self):
            if self.counter < self.limit:
                self.counter += 1
                X = (np.random.randn(self.batch_size, self.input_len) > 0.5).astype(int)
                Y = np.concatenate((X[:, 1:], X[:, 0:1]), axis=1)
                return X, Y
            else:
                raise StopIteration

        def __iter__(self):
            return self

    class SimpleMultiClassifier(nn.Module):
        def __init__(self, num_input, num_output):
            super().__init__()
            self.model = nn.Sequential(
                nn.Linear(num_input, 64),
                nn.ReLU(),
                nn.Linear(64, num_output),
            ) \
                .apply(self.init_weights)

        def forward(self, x):
            return self.model(x)

        @staticmethod
        def init_weights(m):
            if type(m) == nn.Linear:
                torch.nn.init.kaiming_normal_(m.weight)
                # What is it?
                # m.bias.data.fill_(0.01)

    input_size = 10
    output_size = 10

    net = SimpleMultiClassifier(input_size, output_size)

    criterion = nn.MSELoss()
    l_rate = 0.5
    # Stochastic Gradient Descent
    optimizer = torch.optim.SGD(net.parameters(), lr=l_rate)

    epochs = 2000

    # print("X =  \n", X)

    BG = BatchGenerator(input_size, 1000, 10)
    # print("Y = \n", Y)
    for epoch in range(epochs):
        for batch in BG:
            X = (np.random.randn(100, input_size) > 0.5)
            Y = np.concatenate((X[:, 1:], X[:, 0:1]), axis=1)
            inputs = Variable(torch.from_numpy(X.astype(int))).float()
            labels = Variable(torch.from_numpy(Y.astype(int))).float()
            optimizer.zero_grad()

            outputs = net.forward(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            print(f"epoch {epoch}, loss {loss} ")

    X = torch.from_numpy(np.random.randn(10000, input_size) > 0.5).float()
    Y = (net.forward(X) > 0.5).float()

    X = torch.from_numpy(np.concatenate((X[:, 1:], X[:, 0:1]), axis=1)).float()
    Z = (X == Y)
    print(Z[Z != 1])
