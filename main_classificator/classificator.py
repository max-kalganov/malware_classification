from dataclasses import dataclass
from enum import Enum, auto
from typing import List, Tuple

import pandas as pd
from sklearn.neural_network import MLPClassifier
import numpy as np
import torch.nn as nn
import torch
from torch.autograd import Variable

from tqdm import tqdm
from main_classificator import ct
from main_classificator.preprocessing.preproc_data import prepare_train_dataset_pipeline, \
    read_batches_from_test_preprocessed, get_x


def small_batch_classificator():
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes*0.5),
                          int(len_of_bytes*0.2),
                          9)
    clf = simple_model(X, Y, hidden_layers_size=hidden_layers_size)
    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def large_batch_classification():
    # doesn't work, need to reprepare train data, so in each batch will be all classes
    X, Y = prepare_train_dataset_pipeline()
    len_of_bytes = len(X[0])
    print("start training simple model")
    hidden_layers_size = (int(len_of_bytes * 0.5),
                          int(len_of_bytes * 0.2),
                          9,)

    clf = MLPClassifier(activation='tanh', solver='sgd', learning_rate='constant',
                        alpha=1e-4, hidden_layer_sizes=hidden_layers_size, random_state=1,
                        batch_size=1, verbose=False,
                        max_iter=1, warm_start=True)

    clf = clf.partial_fit(X, Y, classes=[1, 2, 3, 4, 5, 6, 7, 8, 9])

    print("start fitting main train dataset")

    for num in tqdm(range(0, 3256)):
        X, Y = prepare_train_dataset_pipeline([num],
                                              path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        clf = clf.partial_fit(X, Y)

    print("start predicting")
    predictions = predict_large(clf)
    predictions.to_csv(ct.get_predictions_new_file())


def aggregate_results(df: pd.DataFrame) -> pd.DataFrame:
    assert ct.PREDICTION_COLUMN in df.columns and ct.ID_COLUMN in df.columns

    # TODO: use a classificator here to get the result classification

    def proc_groups(name, group):
        result_prediction = sum(np.array(list(group[ct.PREDICTION_COLUMN].values))) / len(group)
        return pd.DataFrame({ct.ID_COLUMN: [name],
                             ct.PREDICTION_COLUMN: [result_prediction.tolist()]})

    df = pd.concat([proc_groups(name, group) for name, group in df.groupby([ct.ID_COLUMN])])
    return df


def predict_large(clf) -> pd.DataFrame:
    result_df = pd.DataFrame()
    for df in tqdm(read_batches_from_test_preprocessed(list(range(ct.NUM_OF_TEST_BATCHES)))):
        X_test = get_x(df)
        prediction = clf.predict_proba(X_test)
        df[ct.PREDICTION_COLUMN] = prediction.tolist()
        result_df = pd.concat([result_df, aggregate_results(df)])
    return result_df


def simple_model(X, Y, hidden_layers_size = (5,2)):
    clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
                        hidden_layer_sizes=hidden_layers_size, random_state=1)

    clf.fit(X, Y)
    return clf


def simple_example():
    from random import randint
    # X_train = [[-1., -2.]]
    # y = [0]
    X_train = []
    y = []

    for i in range(100):
        x_ = randint(-10, 10)
        y_ = randint(-10, 10)
        X_train.append([x_, y_])
        if y_ == x_:
            y.append(2)
        else:
            y.append(int(y_ - x_ > 0))

    clf = simple_model(X_train, Y=y, hidden_layers_size=(5, 5, 5))

    X_test = [[0., 0.], [-1., -2.], [-2., -1.], [-1., -1.], [2., 1.]]
    prediction = clf.predict(X_test)
    y_test = [2, 0, 1, 2, 0]
    print(f"prediction result = {prediction}: result is correct - {np.array_equal(prediction, np.array(y_test))}")


####################################
# Writing PyTorch Classificator
####################################

class SimpleMultiClassifier(nn.Module):
    def __init__(self, hidden_layers_size: List[Tuple[int, int]]):
        super().__init__()
        self.hidden_layers_size = hidden_layers_size
        self.hidden_layer_num: int = 0
        self.model = nn.Sequential(
            nn.Linear(*self.__get_next_hidden_layer_size()),
            nn.Sigmoid(),
            nn.Linear(*self.__get_next_hidden_layer_size()),
            nn.Sigmoid()
        )
        #     .apply(self.init_weights)

    def __get_next_hidden_layer_size(self) -> Tuple[int, int]:
        next_size = self.hidden_layers_size[self.hidden_layer_num]
        self.hidden_layer_num += 1
        print(f" hidden layers size = {next_size}")
        return next_size

    def forward(self, x):
        return self.model(x)

    # @staticmethod
    # def init_weights(m):
    #     if type(m) == nn.Linear:
    #         torch.nn.init.kaiming_normal_(m.weight)


@dataclass
class ClassifArgs:
    def __init__(self,
                 num_of_batches: int,
                 hidden_layers: List[Tuple[int, int]],
                 criterion,
                 l_rate: float,
                 percent_of_tasks_to_train: float = 0.75):

        assert num_of_batches > 0
        assert 0 < percent_of_tasks_to_train <= 1, f"percent not in range (0, 1]"
        self.num_of_batches = num_of_batches
        self.hidden_layers = hidden_layers
        self.criterion = criterion
        self.l_rate = l_rate
        self.num_to_train = int(self.num_of_batches * percent_of_tasks_to_train)
        self.num_to_test = self.num_of_batches - self.num_to_train


def pytorch_classification(classif_args: ClassifArgs):
    def train():
        loss = None

        for num in tqdm(range(classif_args.num_to_train)):
            X, Y = prepare_train_dataset_pipeline([num])
            # X, Y = prepare_train_dataset_pipeline([num],
            #                                       path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)

            inputs = Variable(torch.from_numpy(X)).float()
            labels = Variable(torch.from_numpy(Y)).float()
            optimizer.zero_grad()

            outputs = pytorch_NN.forward(inputs)

            loss = classif_args.criterion(outputs, labels)
            loss.backward()
            optimizer.step()

        print(f"loss {loss} ")

    def test():
        X_test, Y_test = prepare_train_dataset_pipeline(
            list(range(classif_args.num_to_train, classif_args.num_of_batches)),
            path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)

        inputs = Variable(torch.from_numpy(X_test.astype(int))).float()
        Y = (pytorch_NN.forward(inputs)).float()

        print(Y)
        np_y = Y.detach().numpy()
        predictions = np.argmax(np_y, axis=1)
        print(Y_test[Y_test == predictions])

    pytorch_NN = SimpleMultiClassifier(hidden_layers_size=classif_args.hidden_layers)

    # Stochastic Gradient Descent
    optimizer = torch.optim.SGD(pytorch_NN.parameters(), lr=classif_args.l_rate)

    train()

    test()


if __name__ == '__main__':
    classif_args = ClassifArgs(
                num_of_batches=65,
                hidden_layers=[(ct.MIN_WINDOW_SIZE, 100), (100, ct.NUM_OF_CLASSES)],
                criterion=nn.MSELoss(),
                l_rate=0.1)
    pytorch_classification(classif_args)
