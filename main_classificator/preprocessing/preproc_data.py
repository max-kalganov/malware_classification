from typing import Mapping, List, Tuple

import pandas as pd
from tqdm import tqdm
from main_classificator import ct
from random import shuffle, sample
import sklearn.preprocessing
import numpy as np

from main_classificator.preprocessing.files_manipulation import get_all_file_names, read_data_from_files, \
    get_num_of_files, read_labels
from main_classificator.utils import Extention, get_file_name_by_extention

def read_data_in_dict(path_to_folder: str,
                      num_of_files: int = None,
                      from_num: int = None,
                      ext: Extention = None,
                      shuffle_list: bool = False) -> Mapping[str, str]:
    file_names = get_all_file_names(path_to_folder)

    if ext:
        file_names = get_file_name_by_extention(file_names, ext)

    if shuffle_list:
        shuffle(file_names)

    if from_num:
        assert from_num >= 0, f"wrong num of from_num parameter = {from_num}"
    else:
        from_num = 0

    if num_of_files:
        assert num_of_files > 0, f"wrong num of files = {num_of_files}"
        file_names = file_names[from_num:from_num + num_of_files]

    all_data = read_data_from_files(file_names)
    return all_data


def transform_bytes_into_np_array(all_data: Mapping[str, str]) -> Mapping[str, np.array]:
    def get_bytes(byte_file: str) -> np.array:
        """

        :param byte_file: str with bytes from file
            template for a row - num_of_a_rowI byte1 byte2 ... byteN\n

            delimeters:
            \n - for rows
            ' ' - for bytes and num of row
        :return:
            for each file_name will be np.array with bytes. -1 is representing QM '?', next num - num of ? goes next
        """

        rows = [cutted_row.split(' ')[1:] for cutted_row in byte_file.split('\n')]
        just_bytes = []
        [just_bytes.extend(r) for r in rows]
        int_bytes = []
        num_of_QM = 0

        for byte in just_bytes:
            if '?' not in byte:
                if num_of_QM != 0:
                    int_bytes.append(-1)
                    int_bytes.append(num_of_QM)
                    num_of_QM = 0

                int_byte = int(byte, 16)
                int_bytes.append(int_byte)
            else:
                num_of_QM += 1

        return np.array(int_bytes)

    transformed_data = {}
    for k, v in all_data.items():
        transformed_data[k] = get_bytes(v)
    return transformed_data


def get_dataset_from_files(path_to_input_folder: str,
                           num_of_files: int = None,
                           from_num: int = None,
                           ext: Extention = None,
                           shuffle_list: bool = False):
    all_data = read_data_in_dict(path_to_folder=path_to_input_folder,
                                 num_of_files=num_of_files,
                                 from_num=from_num,
                                 ext=ext,
                                 shuffle_list=shuffle_list)

    np_data = transform_bytes_into_np_array(all_data)
    return np_data


def get_min_file_len(path_to_folder: str) -> int:
    num_of_files = get_num_of_files(path_to_folder)
    step = 1
    min_len = None
    for i in tqdm(range(0, num_of_files, step)):
        np_dataset = get_dataset_from_files(path_to_input_folder=path_to_folder,
                                            num_of_files=step,
                                            from_num=i,
                                            ext=Extention.BYTES,
                                            shuffle_list=False)
        if not list(np_dataset.values()):
            break
        assert len(list(np_dataset.values())) == 1
        cur_len = len(list(np_dataset.values())[0])
        if min_len is None:
            min_len = cur_len
        elif cur_len < min_len:
            min_len = cur_len
    return min_len


def fit_file_by_size(np_data: Mapping[str, np.array], min_len: int, overlap: int) -> pd.DataFrame:
    # TODO: check this, if it gives bad results, then just use reshape
    res_df = pd.DataFrame()
    for file_name, cur_bytes in np_data.items():
        cur_bytes = cur_bytes.tolist()
        cur_res = []
        len_of_bytes = len(cur_bytes)
        if len_of_bytes < min_len:
            print(f"file which size is too small = {file_name}")
            res_df = pd.concat([res_df, pd.DataFrame({ct.ID_COLUMN: [file_name],
                                                      ct.BYTES_COLUMN: [cur_bytes + [-100] * (min_len - len_of_bytes)]})])
        else:
            for i in range(0, len_of_bytes, min_len-overlap):
                cur_dataset = cur_bytes[i:i+min_len]
                if len(cur_dataset) == min_len:
                    cur_res.append(cur_dataset)
                else:
                    dataset_to_check = cur_bytes[len_of_bytes - min_len:len_of_bytes]
                    if not np.array_equal(dataset_to_check, cur_res[-1]):
                        cur_res.append(dataset_to_check)
            res_df = pd.concat([res_df, pd.DataFrame({ct.ID_COLUMN: [file_name]*len(cur_res),
                                                      ct.BYTES_COLUMN: cur_res})])
    return res_df


def add_labels(df: pd.DataFrame) -> pd.DataFrame:
    def format_ids(id_name: str):
        return id_name.split('.')[0]

    assert ct.ID_COLUMN in df.columns and ct.BYTES_COLUMN in df.columns, "no correct columns in df"
    labels_df = read_labels(ct.get_path_to_train_labels_csv())
    df[ct.ID_COLUMN] = df[ct.ID_COLUMN].apply(format_ids)
    # TODO: check if names are changed in df
    res_df = pd.merge(left=df, right=labels_df, on=ct.ID_COLUMN, how='left')
    assert len(df) == len(res_df), "wrong merge"
    return res_df


def get_x(df) -> np.array:
    import json
    assert ct.ID_COLUMN in df.columns, "ID column not in df"
    assert ct.BYTES_COLUMN in df.columns, "bytes column not in df"
    return np.array([json.loads(bytes_array_in_str) for bytes_array_in_str in list(df[ct.BYTES_COLUMN].values)])


def get_y(df) -> np.array:
    assert ct.ID_COLUMN in df.columns, "ID column not in df"
    assert ct.CLASS_COLUMN in df.columns, "class column not in df"
    return np.array(list(df[ct.CLASS_COLUMN].values))


def get_x_y(df: pd.DataFrame) -> Tuple[np.array, np.array]:
    X = get_x(df)
    Y = get_y(df)
    return X, Y


def write_preprocessed_data(path_to_input_folder: str,
                            path_to_output_file: str,
                            min_len: int,
                            overlap: int,
                            num_of_files: int = None,
                            from_num: int = None,
                            ext: Extention = None,
                            shuffle_list: bool = False):

    np_data = get_dataset_from_files(path_to_input_folder, num_of_files, from_num, ext, shuffle_list)
    res_df = fit_file_by_size(np_data, min_len, overlap)
    res_df.to_csv(path_to_output_file)


def read_batches_from_train_preprocessed(nums_of_batches_to_read: List[int],
                                         path_to_file_generator=ct.get_path_to_train_preprocessed_file) -> pd.DataFrame:
    assert nums_of_batches_to_read
    for batch_num in nums_of_batches_to_read:
        yield pd.read_csv(path_to_file_generator(batch_num))


def read_batches_from_test_preprocessed(nums_of_batches_to_read: List[int]) -> pd.DataFrame:
    assert nums_of_batches_to_read
    for batch_num in nums_of_batches_to_read:
        yield pd.read_csv(ct.get_path_to_test_preprocessed_file(batch_num))


def preproc_dataset_in_batches(input_folder: str = ct.get_path_to_train_cut_folder(),
                               output_file_names_generator=ct.get_path_to_train_preprocessed_file,
                               start_batch_num: int = 0):
    num_of_files = get_num_of_files(input_folder, ext=Extention.BYTES)
    step = 3
    num_of_batch = 0
    for i in tqdm(range(start_batch_num, num_of_files, step)):
        write_preprocessed_data(path_to_input_folder=input_folder,
                                path_to_output_file=output_file_names_generator(num_of_batch),
                                min_len=ct.MIN_WINDOW_SIZE,
                                overlap=ct.OVERLAP,
                                num_of_files=step,
                                from_num=i,
                                ext=Extention.BYTES,
                                shuffle_list=False)
        num_of_batch += 1


def format_to_one_hot(labels: np.array, num_of_classes: int = ct.NUM_OF_CLASSES) -> np.array:
    label_binarizer = sklearn.preprocessing.LabelBinarizer()
    label_binarizer.fit(range(1, num_of_classes+1))
    ont_hot_vect = label_binarizer.transform(labels)
    return ont_hot_vect


def get_batch_nums_by_labels_from_train(num_of_samples_for_each: int,
                                        label_nums_to_get: List[int] = None) -> List[int]:
    if not label_nums_to_get:
        label_nums_to_get = list(range(1, ct.NUM_OF_CLASSES + 1))

    label_file_name_map = {k: [] for k in label_nums_to_get}

    for num in tqdm(range(ct.TRAIN_FULL_PREPROCESSED_SIZE)):
        res_df = read_batches_with_labels([num],
                                          path_to_train_file_generator=ct.get_path_to_train_full_preprocessed_file)
        labels = set(res_df[ct.CLASS_COLUMN])
        for label in labels:
            label_file_name_map[label].append(num)

    result_batch_nums = set()
    for label, nums in label_file_name_map.items():
        result_batch_nums.update(sample(nums, num_of_samples_for_each))
    return list(result_batch_nums)


def get_batch_num_by_label(num_of_samples_for_each: int,
                           label_nums_to_get: List[int] = None) -> List[int]:
    def select_sample(row: pd.Series) -> pd.Series:
        import json
        if row[ct.CLASS_COLUMN.lower()] in label_nums_to_get:
            result_batch_nums.update(sample(json.loads(row["batch_nums_with_class"]), num_of_samples_for_each))
        return row

    if not label_nums_to_get:
        label_nums_to_get = list(range(1, ct.NUM_OF_CLASSES + 1))

    labels_file_name_map = pd.read_csv(ct.get_path_to_label_batch_num_map())

    result_batch_nums = set()

    labels_file_name_map.apply(select_sample, axis=1)

    return list(result_batch_nums)


def read_batches_with_labels(numbers_of_batches: List[int] = None,
                             path_to_train_file_generator=ct.get_path_to_train_preprocessed_file):
    if not numbers_of_batches:
        numbers_of_batches = list(range(65))

    res_df = pd.DataFrame()
    for df in read_batches_from_train_preprocessed(numbers_of_batches,
                                                   path_to_file_generator=path_to_train_file_generator):
        res_df = pd.concat([res_df, df])

    res_df = add_labels(res_df)
    return res_df


def prepare_train_dataset_pipeline(numbers_of_batches: List[int] = None,
                                   path_to_train_file_generator=ct.get_path_to_train_preprocessed_file) -> Tuple[np.array, np.array]:
    res_df = read_batches_with_labels(numbers_of_batches, path_to_train_file_generator)

    X, Y = get_x_y(res_df)
    Y = format_to_one_hot(Y)
    return X, Y


if __name__ == '__main__':
    print(get_batch_num_by_label(num_of_samples_for_each=3, label_nums_to_get=[1]))

    # preproc_dataset_in_batches(input_folder=ct.get_path_to_train_folder(),
    #                            output_file_names_generator=ct.get_path_to_train_full_preprocessed_file,
    #                            start_batch_num=1100)
